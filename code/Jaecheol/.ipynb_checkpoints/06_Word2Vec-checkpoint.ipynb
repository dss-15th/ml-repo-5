{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from khaiii import KhaiiiApi; k = KhaiiiApi()\n",
    "from konlpy.tag import Okt; t = Okt()\n",
    "from konlpy.utils import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./new_train.csv')\n",
    "test = pd.read_csv('../../data/dev.hate.csv')\n",
    "train = raw_data.copy()\n",
    "\n",
    "X_train = train['comments']\n",
    "X_test = test['comments']\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 위한 댓글 데이터(200만개) 로드 후 10만개만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지드래곤은 난봉꾼이란...댓글도 달렸네 ㅋㅋ 이주연 학창시절 사진 보고 와라. 요즘 웬만한 여자 연예인하고 붙여놔도....미모가 최고였단다.ㅋ 5대 얼짱 출신.',\n",
       " '이주연은 알겠는데 지디는 뭐하는 듣보잡여',\n",
       " '부럽네요. 나도 불과 한달전까진 허니문베이비를 꿈꿨는데 이제 다 부질없네요. 당연히 순결할거라 믿었고 그래서 첫날밤까지 기다려준건데 배신감만 듭니다. 첫날밤 와이프가 피를 안흘렸어요. 처가집식구들이 일부러 절 속였단 생각에 화도나고 어제 처가집가기로 했는데 안간다고 했더니 혼자 울고 갔다와서 지금까지 한마디도 안해요. 이혼하고 싶네요',\n",
       " '\"이주연을 모르는 애들이 많네. 해체된 애프터스쿨 멤버로 당시는 주연이 예명. 인기나 포텐은 안터졌으나, 순수미모만으로는 애프터스쿨에서 원탑이었다. 진짜 자연미인이다.\"',\n",
       " '겨론했으면',\n",
       " '이주연이 아깝다 진심',\n",
       " '방탄은 건드리지말자 인간적으로.. 해외활동 지금주터 시작인데 터지면 진짜 전 세계적으로 활짝 피기도 전에 난리난다.. 국위선양하는 떠오르는 샛별은 제발 건드리지말고.. 탄이들도 만약 연애하더라도 들키지 않길.. ㅠㅠ',\n",
       " '선남선녀의 만남이네요^^',\n",
       " '문재앙 또뭘덥고 싶어서 ㄷㄷㄷ',\n",
       " '주연이 아깝긴해요']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../../korean-hate-speech-detection/unlabeled_comments.txt', 'r') as f:\n",
    "    text = f.readlines()\n",
    "text = [txt.replace(\"\\n\", \"\") for txt in text]\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034837"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cc78560077f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msoynlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# train['comments'] = [repeat_normalize(comment, num_repeats=2) for comment in train['comments']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrepeat_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-cc78560077f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msoynlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# train['comments'] = [repeat_normalize(comment, num_repeats=2) for comment in train['comments']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrepeat_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/soynlp/normalizer/_normalizer.py\u001b[0m in \u001b[0;36mrepeat_normalize\u001b[0;34m(sent, num_repeats)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepeat_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_repeats\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeatchars_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\1'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoublespace_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# 특수문자 제거\n",
    "import re\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)\n",
    "    return text\n",
    "\n",
    "text = [cleanse(txt) for txt in text]\n",
    "\n",
    "# 띄어쓰기\n",
    "from pykospacing import spacing\n",
    "text = [spacing(txt) for txt in text]\n",
    "\n",
    "# 문장 분리\n",
    "import kss\n",
    "# train['comments'] = train['comments'].apply(kss.split_sentences)\n",
    "# train['comments'] = [','.join(map(str, ls)) for ls in train['comments']]\n",
    "text = [kss.split_sentences(txt) for txt in text]\n",
    "\n",
    "# 중복 제거\n",
    "from soynlp.normalizer import *\n",
    "# train['comments'] = [repeat_normalize(comment, num_repeats=2) for comment in train['comments']]\n",
    "text = [repeat_normalize(txt, num_repeats=2) for txt in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for sentence in text:\n",
    "    tokenized_data.append(t.morphs(sentence, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = word2vec.Word2Vec(tokenized_data, size=100, window=5, min_count=5, workers=4)\n",
    "embedding.save('./comments100.model')\n",
    "# sentences : 각 문장 마다 하나의 토큰 list를 생성하며 토큰 list의 개수는 문장 개수 n개 만큼 생성하여 sentences에 저장해둠\n",
    "# size : word 벡터 차원\n",
    "# window : 현재 단어와 예측 단어의 최대 거리\n",
    "# negative : 0으로 두면 negative smapling을 하지하지 않으며, 0보다 큰 값이면 negative sampling을 수행함\n",
    "# min_count : min_count의 빈도수보다 낮은 빈도수인 단어는 무시합니다.\n",
    "# worker : 모델 생성시 사용할 쓰레드 개수\n",
    "# sg : sg 값이 1이면 skip-gram이지만 그렇지 않으면 CBOW 알고리즘을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./comments100.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('연옌', 0.874727725982666),\n",
       " ('일반인', 0.8209106922149658),\n",
       " ('유명인', 0.7678678631782532),\n",
       " ('옌', 0.7546504735946655),\n",
       " ('옌예인', 0.7006750106811523),\n",
       " ('연예', 0.6997705698013306),\n",
       " ('정치인', 0.6985364556312561),\n",
       " ('스포츠스타', 0.6967839002609253),\n",
       " ('방송인', 0.6937679052352905),\n",
       " ('공인', 0.692998468875885),\n",
       " ('직종', 0.6905813813209534),\n",
       " ('옌예', 0.6840217113494873),\n",
       " ('인들', 0.6760807633399963),\n",
       " ('자숙중', 0.6717374324798584),\n",
       " ('회사원', 0.670289158821106),\n",
       " ('개제', 0.6488688588142395),\n",
       " ('예인', 0.6410459876060486),\n",
       " ('먹고살다', 0.6362445950508118),\n",
       " ('연얜', 0.6356096267700195),\n",
       " ('스포츠선수', 0.6282926201820374),\n",
       " ('아이돌', 0.6272568106651306),\n",
       " ('면지', 0.625293493270874),\n",
       " ('개그맨', 0.6183145046234131),\n",
       " ('지망', 0.6176378130912781),\n",
       " ('열녀', 0.6168718934059143),\n",
       " ('직업여성', 0.6168108582496643),\n",
       " ('소음인', 0.6137609481811523),\n",
       " ('뻘짓해', 0.6135314702987671),\n",
       " ('엄친딸', 0.6126554608345032),\n",
       " ('유명인사', 0.6119839549064636)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"연예인\"], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'586',\n",
       " '엄만데',\n",
       " '지장',\n",
       " '한성',\n",
       " '층',\n",
       " '독일인',\n",
       " '미쏘',\n",
       " '노엘',\n",
       " '꺼리다',\n",
       " '수파',\n",
       " '후퇴',\n",
       " 'ㅅㅅㅅ',\n",
       " '신병',\n",
       " '개국',\n",
       " '휘성',\n",
       " '걸데',\n",
       " '취미',\n",
       " '축구',\n",
       " '애고',\n",
       " '방송위',\n",
       " '도곡동',\n",
       " '강미선',\n",
       " '이윤석',\n",
       " '잉꼬',\n",
       " '윤진이',\n",
       " '뼈',\n",
       " '손오공',\n",
       " '대냐',\n",
       " '해탈',\n",
       " '권민중',\n",
       " '워커',\n",
       " 'da',\n",
       " '자하',\n",
       " '툴툴거리',\n",
       " '부국',\n",
       " '조서',\n",
       " '허세',\n",
       " '재적',\n",
       " '빠충',\n",
       " '덕실',\n",
       " '시크릿',\n",
       " '쪽파',\n",
       " '성화',\n",
       " '울적하다',\n",
       " '아뒤',\n",
       " '117',\n",
       " '레젼드',\n",
       " '골빈녀',\n",
       " '복서',\n",
       " '백건',\n",
       " '사무소',\n",
       " '양분',\n",
       " '꼴같잖다',\n",
       " '모던',\n",
       " '코스튬',\n",
       " '장희진',\n",
       " '다담',\n",
       " '븅들',\n",
       " '들뜨다',\n",
       " '증언',\n",
       " '인전',\n",
       " '네이처',\n",
       " '대결',\n",
       " '쟤내',\n",
       " '고유라',\n",
       " '능률',\n",
       " '정아',\n",
       " '틴탑',\n",
       " '키드',\n",
       " '고추장',\n",
       " '박경아',\n",
       " '엊',\n",
       " '남상',\n",
       " '하내',\n",
       " '없쟈',\n",
       " '더킹',\n",
       " '비걸',\n",
       " '한정수',\n",
       " '마세라티',\n",
       " '스캔들',\n",
       " '대구',\n",
       " '세월호',\n",
       " '심은하',\n",
       " '호적',\n",
       " '방수',\n",
       " '달총',\n",
       " '유희열',\n",
       " '시랭',\n",
       " '묵시',\n",
       " '느릿느릿',\n",
       " '방사선',\n",
       " '녹취',\n",
       " '신성하다',\n",
       " '슷',\n",
       " '망국',\n",
       " '역기',\n",
       " '개셈',\n",
       " '망붕이',\n",
       " '사녹',\n",
       " '유권',\n",
       " '진보',\n",
       " '출마',\n",
       " '꾸준하다',\n",
       " '코맹맹',\n",
       " 'ㅓ',\n",
       " '재소',\n",
       " '도참',\n",
       " '단백',\n",
       " '이끼',\n",
       " '전반전',\n",
       " '억양',\n",
       " '바알',\n",
       " 'sue',\n",
       " '뭐랬',\n",
       " '일어',\n",
       " '가내',\n",
       " '정몽준',\n",
       " '피드백',\n",
       " '덤',\n",
       " '그간',\n",
       " '보기',\n",
       " '이현우',\n",
       " '기술',\n",
       " '마법',\n",
       " '괴롭히다',\n",
       " '이대원',\n",
       " '쓰레',\n",
       " '치년',\n",
       " '할머니',\n",
       " '지학',\n",
       " '지못미',\n",
       " '달아오르다',\n",
       " '비빔면',\n",
       " '입쪽',\n",
       " '솔잎',\n",
       " '항문성교',\n",
       " '아가씨',\n",
       " '어이구',\n",
       " '뉘',\n",
       " '미스터리',\n",
       " '얘',\n",
       " '이하율',\n",
       " '다코야키',\n",
       " '발걸음',\n",
       " '포션',\n",
       " '현',\n",
       " '독무',\n",
       " '오즘',\n",
       " '얼귤',\n",
       " '오래오래',\n",
       " '용서',\n",
       " '재취',\n",
       " '수가',\n",
       " '입장권',\n",
       " '골고루',\n",
       " '반세기',\n",
       " '려무나',\n",
       " '환경오염',\n",
       " '졸혼',\n",
       " '매몰',\n",
       " '스타벅스',\n",
       " '푹',\n",
       " '지면',\n",
       " '홈피',\n",
       " '목적',\n",
       " '때문에',\n",
       " '콩가루',\n",
       " '포자',\n",
       " '좌국',\n",
       " '들이',\n",
       " '천금',\n",
       " '김도연',\n",
       " '알박기',\n",
       " '메너',\n",
       " '잔인',\n",
       " '어차피',\n",
       " '지름길',\n",
       " '임인',\n",
       " '작곡',\n",
       " '이태리',\n",
       " '쓰나미',\n",
       " '버려지다',\n",
       " '선시',\n",
       " '조정선',\n",
       " '능데',\n",
       " '멋찜',\n",
       " '말르다',\n",
       " '주저리주저리',\n",
       " '부실하다',\n",
       " 'ㄴㅋㅋ',\n",
       " '자꼬',\n",
       " '끈기',\n",
       " '팼',\n",
       " '상여',\n",
       " '여자',\n",
       " '영양가',\n",
       " 'ㄷㅊ',\n",
       " '눈새',\n",
       " '다해',\n",
       " '환수',\n",
       " 'nono',\n",
       " '아리까리하다',\n",
       " '폐간',\n",
       " '장통',\n",
       " '비참',\n",
       " '우욱',\n",
       " '휘트니',\n",
       " '퍼거슨',\n",
       " 'awesome',\n",
       " '될껍니',\n",
       " '겸손',\n",
       " '시윤',\n",
       " '심리학자',\n",
       " '적용',\n",
       " '언프',\n",
       " '삼봉',\n",
       " '바함',\n",
       " '막장',\n",
       " '보행기',\n",
       " '킹',\n",
       " '탄광',\n",
       " '달님',\n",
       " '반바지',\n",
       " '됏음',\n",
       " '센스',\n",
       " '품절',\n",
       " '가을동화',\n",
       " '론칭',\n",
       " '왓비컴즈',\n",
       " '게이버',\n",
       " '개정',\n",
       " '펭하',\n",
       " '쓸모',\n",
       " '여인',\n",
       " '스승',\n",
       " '백선생',\n",
       " '건국',\n",
       " '정희',\n",
       " '뱅뱅',\n",
       " '정하은',\n",
       " '중탕',\n",
       " '넵',\n",
       " '임세령',\n",
       " '끓이다',\n",
       " '넝쿨당',\n",
       " '키위',\n",
       " '선행',\n",
       " '테라스',\n",
       " '부패',\n",
       " '산본',\n",
       " 'pdf',\n",
       " '흐뭇다',\n",
       " '가은이',\n",
       " '틀니',\n",
       " '성미',\n",
       " '기근',\n",
       " 'hard',\n",
       " '근로',\n",
       " '해명',\n",
       " '83년',\n",
       " '단락',\n",
       " '051',\n",
       " '지랠',\n",
       " '주근깨',\n",
       " '라커',\n",
       " '재경',\n",
       " '급부',\n",
       " '상근이',\n",
       " '김민희',\n",
       " '전화',\n",
       " '시샘',\n",
       " '예토전생',\n",
       " '집유끝',\n",
       " '정적',\n",
       " '위중하다',\n",
       " '효력',\n",
       " '문주란',\n",
       " '글렀어',\n",
       " '용한',\n",
       " '놨',\n",
       " '노여움',\n",
       " 'ㅋㅋㅋㅌ',\n",
       " '췌장암',\n",
       " '난건',\n",
       " '도처',\n",
       " '짧다',\n",
       " '먼데이',\n",
       " '달밥값',\n",
       " '김민우',\n",
       " '부럽드',\n",
       " '아모',\n",
       " '왜깜',\n",
       " '멋졍',\n",
       " '활성화',\n",
       " '말뽄새',\n",
       " '밴쯔',\n",
       " 'google',\n",
       " '웰',\n",
       " 'us',\n",
       " '레베카',\n",
       " 'ㅜㅡㅜ',\n",
       " '군림',\n",
       " '스포츠',\n",
       " '확진',\n",
       " '워리',\n",
       " '신에',\n",
       " '독점',\n",
       " '김지우',\n",
       " '헤롱헤롱',\n",
       " '사촌형',\n",
       " 'leexohyun',\n",
       " '구준',\n",
       " 'md',\n",
       " '매드클라운',\n",
       " '국문과',\n",
       " '크아',\n",
       " '란걸',\n",
       " '교우',\n",
       " '9000원',\n",
       " '횡재',\n",
       " '겧',\n",
       " '제출',\n",
       " 'some',\n",
       " '싸움판',\n",
       " '50년',\n",
       " '해결사',\n",
       " '메우다',\n",
       " '거닐다',\n",
       " '수색대',\n",
       " '지상렬',\n",
       " '선지',\n",
       " '개핵',\n",
       " '가트',\n",
       " '레기들',\n",
       " '싹싹',\n",
       " '울상',\n",
       " '바짝',\n",
       " '생겻다',\n",
       " '독립군',\n",
       " '중절',\n",
       " '조리사',\n",
       " '변덕',\n",
       " '인현왕후',\n",
       " '준말',\n",
       " '반미',\n",
       " '과로',\n",
       " '전폐',\n",
       " '십분',\n",
       " '들들',\n",
       " '핏대',\n",
       " '어께',\n",
       " '아리마',\n",
       " '샤를',\n",
       " '박보영',\n",
       " '스크래치',\n",
       " '초벌',\n",
       " '효주',\n",
       " '유별나다',\n",
       " '부비부비',\n",
       " '장모님',\n",
       " 'jo',\n",
       " '송',\n",
       " '붉다',\n",
       " '기행',\n",
       " '돗',\n",
       " '급비',\n",
       " '할기다',\n",
       " '안엮',\n",
       " '스프링',\n",
       " '니키타',\n",
       " '피부',\n",
       " '숏다리',\n",
       " '이맘때',\n",
       " 'ㄲㅓ',\n",
       " '녀',\n",
       " '무한',\n",
       " '섹쉬',\n",
       " '렇',\n",
       " '빋',\n",
       " '지난해',\n",
       " '할로',\n",
       " '상황',\n",
       " '국보',\n",
       " '리안',\n",
       " '교정',\n",
       " '중생',\n",
       " '다누',\n",
       " '비아그라',\n",
       " '반',\n",
       " '311',\n",
       " 'full',\n",
       " '정공',\n",
       " '첨으로',\n",
       " '죄국',\n",
       " 'prince',\n",
       " '씹극혐',\n",
       " '좃되',\n",
       " '조력자',\n",
       " '예쁘다',\n",
       " '들락날락',\n",
       " '육박',\n",
       " '건립',\n",
       " '메갈들',\n",
       " '매하',\n",
       " '러브스토리',\n",
       " '나이팅게일',\n",
       " 'channel',\n",
       " '못감',\n",
       " '하층',\n",
       " '쾡',\n",
       " '양지원',\n",
       " '안전하다',\n",
       " '단원고',\n",
       " '언능',\n",
       " '병안',\n",
       " '양현종',\n",
       " '직관',\n",
       " '표시',\n",
       " '뉘규',\n",
       " '산타',\n",
       " '국숫집',\n",
       " '내나라',\n",
       " '무모하다',\n",
       " '동행',\n",
       " '쓰러지다',\n",
       " '신세경',\n",
       " '식혜',\n",
       " '겠다',\n",
       " '가수',\n",
       " '쇄국정책',\n",
       " '클린',\n",
       " '스벅',\n",
       " '악인',\n",
       " '펼치다',\n",
       " '혈기',\n",
       " '시무',\n",
       " '상하이',\n",
       " '랼',\n",
       " '안지만',\n",
       " '이전에',\n",
       " '사소하다',\n",
       " '뚫어지다',\n",
       " '칭하',\n",
       " '발칙하다',\n",
       " 'k',\n",
       " '신성',\n",
       " '재력',\n",
       " '갓세븐',\n",
       " '앨범',\n",
       " '웃어요',\n",
       " 'if',\n",
       " '지주연',\n",
       " '지혜롭',\n",
       " '라스',\n",
       " '한구석',\n",
       " '유캔',\n",
       " '관종임',\n",
       " '저렇게',\n",
       " '혼잣말',\n",
       " '잠실',\n",
       " '마음잡다',\n",
       " '사후세계',\n",
       " '어지럽다',\n",
       " '이주연',\n",
       " '저작물',\n",
       " '찰리푸스',\n",
       " '섬',\n",
       " '컨텐츠',\n",
       " '하여튼',\n",
       " '평점',\n",
       " '린치',\n",
       " '전향',\n",
       " '얘땜',\n",
       " '터트린것',\n",
       " '블리',\n",
       " '위대',\n",
       " '조영구',\n",
       " '정당하다',\n",
       " '2009',\n",
       " '김용국',\n",
       " '썻던',\n",
       " '멈추다',\n",
       " '표심',\n",
       " '핫',\n",
       " '시마',\n",
       " '밷',\n",
       " '두째',\n",
       " '장군',\n",
       " '버닝',\n",
       " '원빈',\n",
       " '배시시',\n",
       " '마눌',\n",
       " '몇장',\n",
       " '족',\n",
       " '강신일',\n",
       " '미혼',\n",
       " '매크로',\n",
       " '개월때',\n",
       " '감정이입',\n",
       " '가족',\n",
       " '마요네즈',\n",
       " '대략',\n",
       " '평상시',\n",
       " '붕괴',\n",
       " '주병',\n",
       " '도장',\n",
       " '티켓팅',\n",
       " '말',\n",
       " '진출',\n",
       " '딸리다',\n",
       " 'show',\n",
       " '사실관계',\n",
       " 'was',\n",
       " 'please',\n",
       " '판하',\n",
       " '교육감',\n",
       " '절대로',\n",
       " '갈림길',\n",
       " '애먼',\n",
       " '러신',\n",
       " '마여',\n",
       " '나랏돈',\n",
       " '이윤택',\n",
       " '유뷰남',\n",
       " '달팽이',\n",
       " '파프리카',\n",
       " '안봣음',\n",
       " '여분',\n",
       " '꿍꿍',\n",
       " '뼈쌤',\n",
       " '야동',\n",
       " '어진',\n",
       " '아등바등',\n",
       " '토속',\n",
       " '한숨',\n",
       " '퀸',\n",
       " '어불성설',\n",
       " '진술',\n",
       " '풍문',\n",
       " '누규',\n",
       " '도야',\n",
       " '동시',\n",
       " '비위',\n",
       " '파급',\n",
       " '히트치다',\n",
       " '성형',\n",
       " '나열',\n",
       " '짓안',\n",
       " '가기',\n",
       " '밀가루',\n",
       " '사직서',\n",
       " 'ㅇㅉㄹㄱ',\n",
       " '스매싱',\n",
       " '캣맘',\n",
       " '버스킹',\n",
       " 'Netflix',\n",
       " '금욜',\n",
       " '코걸이',\n",
       " '조작상',\n",
       " '시상',\n",
       " '미군',\n",
       " '삼일절',\n",
       " '직인',\n",
       " '노미네이트',\n",
       " '만을',\n",
       " '오글거리다',\n",
       " '비비고',\n",
       " '라피',\n",
       " '수백만',\n",
       " '농단',\n",
       " '전씨',\n",
       " '통로',\n",
       " '딲',\n",
       " '레인지',\n",
       " '젓가락',\n",
       " '정',\n",
       " '쿨러',\n",
       " '라티',\n",
       " '즌',\n",
       " '자빠트리',\n",
       " '명예훼손',\n",
       " '볼륨',\n",
       " '우의',\n",
       " '근로기준법',\n",
       " '소득',\n",
       " '통솔',\n",
       " '사투',\n",
       " '감우성',\n",
       " '한명인',\n",
       " '동갑',\n",
       " '안',\n",
       " '시브',\n",
       " '습하다',\n",
       " '끼',\n",
       " '배용준',\n",
       " '곁눈질',\n",
       " '크레파스',\n",
       " '컨드',\n",
       " '성기',\n",
       " '직찍',\n",
       " '전자파',\n",
       " '돼다',\n",
       " '배달',\n",
       " '무',\n",
       " '그나저나',\n",
       " '봉',\n",
       " '남캐',\n",
       " '디시인사이드',\n",
       " '이수만',\n",
       " '별다르다',\n",
       " '느므',\n",
       " '이기심',\n",
       " '차분하다',\n",
       " '류태준',\n",
       " '필링',\n",
       " '사고팔다',\n",
       " '상남자',\n",
       " '버핏',\n",
       " '장단점',\n",
       " '옜다',\n",
       " '만물',\n",
       " '다압니',\n",
       " '쉐기',\n",
       " '다음가다',\n",
       " '타히티',\n",
       " '돈말',\n",
       " '생기발랄하다',\n",
       " '갯',\n",
       " '만해',\n",
       " '승훈',\n",
       " '육아휴직',\n",
       " '주검',\n",
       " '재밋엇음',\n",
       " '김우성',\n",
       " '곳애',\n",
       " '불우하다',\n",
       " '환장하다',\n",
       " '잡아먹다',\n",
       " '끈끈',\n",
       " '불쌍',\n",
       " '스티',\n",
       " '박지원',\n",
       " '담호',\n",
       " 'wrong',\n",
       " '대일',\n",
       " '에스코트',\n",
       " '오메',\n",
       " '밬',\n",
       " '초상',\n",
       " '으느',\n",
       " '부림',\n",
       " '호남',\n",
       " '재아',\n",
       " '필로폰',\n",
       " '어도',\n",
       " '남부',\n",
       " '답사',\n",
       " '사랑방',\n",
       " '이상아',\n",
       " '박민정',\n",
       " 'Sk',\n",
       " '선영',\n",
       " '일반병사',\n",
       " '오대',\n",
       " '싯점',\n",
       " '그깐',\n",
       " '시켯다',\n",
       " 'oo',\n",
       " '내일',\n",
       " '정정당당하다',\n",
       " '6월',\n",
       " '원통',\n",
       " '좌빠',\n",
       " '쉴드치다',\n",
       " '음기',\n",
       " '새치기',\n",
       " '나이',\n",
       " '격렬하다',\n",
       " '식겁하다',\n",
       " '단타',\n",
       " '아깝다',\n",
       " '7년',\n",
       " 'EBS',\n",
       " '부조리',\n",
       " '자신',\n",
       " '개폼',\n",
       " '민선',\n",
       " '기소',\n",
       " '생소하다',\n",
       " '엮어',\n",
       " '원희룡',\n",
       " '커플링',\n",
       " '불운',\n",
       " '100만',\n",
       " 'ㅋㅋㄱㅋ',\n",
       " '맘에안듬',\n",
       " '피부색',\n",
       " 'muy',\n",
       " '명월',\n",
       " '미달',\n",
       " '트윗',\n",
       " '장발장',\n",
       " '킥킥',\n",
       " '추혜선',\n",
       " '끼어들다',\n",
       " '120만원',\n",
       " '이경',\n",
       " '명복',\n",
       " '카더라',\n",
       " '혜경',\n",
       " '명정',\n",
       " '파다하다',\n",
       " '탈덕해',\n",
       " '의원',\n",
       " '류준',\n",
       " '100회',\n",
       " '증조',\n",
       " '닝겐',\n",
       " '성균관대',\n",
       " '청소년기',\n",
       " '룰라',\n",
       " '00',\n",
       " '섹션티비',\n",
       " '사당',\n",
       " '만지작',\n",
       " 'JJ',\n",
       " '파의',\n",
       " '유난스럽다',\n",
       " 'Article',\n",
       " '여중',\n",
       " '가전제품',\n",
       " '괸찮',\n",
       " '좌파정치',\n",
       " '이였는데',\n",
       " '개화',\n",
       " '반성은',\n",
       " '묵직하다',\n",
       " '세종대왕',\n",
       " '고전',\n",
       " '좌판',\n",
       " '골프대회',\n",
       " '곰팡이',\n",
       " '검머외',\n",
       " '정의도',\n",
       " '애뜻',\n",
       " '물증',\n",
       " '당당하다',\n",
       " '신효범',\n",
       " '선샤인',\n",
       " '내동',\n",
       " '보석',\n",
       " '만원',\n",
       " '이왕표',\n",
       " '양지',\n",
       " '브라만',\n",
       " '오해사',\n",
       " '김준',\n",
       " '직구',\n",
       " '만이라도',\n",
       " '경규',\n",
       " '피뎁',\n",
       " 'Lj',\n",
       " '예쁘장하다',\n",
       " '내보내다',\n",
       " '기장',\n",
       " '자살',\n",
       " '버스정류장',\n",
       " '띨빵',\n",
       " '매니',\n",
       " '역할',\n",
       " '박종훈',\n",
       " '엿먹일',\n",
       " '추격자',\n",
       " '중세시대',\n",
       " '엔',\n",
       " '먹방',\n",
       " '종놈',\n",
       " '참낰',\n",
       " '송영철',\n",
       " '안내서',\n",
       " '맛집',\n",
       " '운전대',\n",
       " '니엘',\n",
       " '구가의서',\n",
       " '연건',\n",
       " '맘좀',\n",
       " '비주',\n",
       " '파고',\n",
       " '피카츄',\n",
       " '주보',\n",
       " '대현',\n",
       " '전광판',\n",
       " '85만',\n",
       " '틀다',\n",
       " '박은빈',\n",
       " '버렷',\n",
       " '쪼옴',\n",
       " '번주',\n",
       " '윤균상님',\n",
       " '허위기사',\n",
       " '카니발',\n",
       " '살림',\n",
       " '저녁',\n",
       " 'msg',\n",
       " '대고',\n",
       " '도그',\n",
       " '격어',\n",
       " '과일',\n",
       " '일라나',\n",
       " '비제',\n",
       " '살좀찌',\n",
       " 'auf',\n",
       " '맹바기',\n",
       " '떨어지다',\n",
       " '애때',\n",
       " '개솔',\n",
       " '해롭다',\n",
       " '꾸러기',\n",
       " '8등',\n",
       " '구글링',\n",
       " '후유증',\n",
       " '탈피',\n",
       " '다음',\n",
       " '안젤라',\n",
       " '피에스타',\n",
       " '트리나',\n",
       " '잡아내다',\n",
       " '복받치다',\n",
       " '정은',\n",
       " '평등',\n",
       " '산업화',\n",
       " '상대성',\n",
       " '인지도',\n",
       " '3040',\n",
       " '노노',\n",
       " '현질',\n",
       " '렉시',\n",
       " '유부초밥',\n",
       " '지브리',\n",
       " '되돌아가다',\n",
       " '푸드코트',\n",
       " '모르겄다',\n",
       " '열녀',\n",
       " '하고도',\n",
       " '무쌍',\n",
       " '사차원',\n",
       " '오세근',\n",
       " '인터',\n",
       " 'oecd',\n",
       " '무력하다',\n",
       " '스탭',\n",
       " 'ㄷㅐ',\n",
       " '뒤늦다',\n",
       " '폭리',\n",
       " '윤석열',\n",
       " '김용호',\n",
       " '어간',\n",
       " '아동',\n",
       " '인직',\n",
       " '다녀가다',\n",
       " '19',\n",
       " '공간',\n",
       " '독자',\n",
       " '인규',\n",
       " '놀리다',\n",
       " '중도하차',\n",
       " '리창',\n",
       " '우즈',\n",
       " '출범',\n",
       " 'ㅋㅌㅌ',\n",
       " '채좀',\n",
       " '가까워지다',\n",
       " '맞팔',\n",
       " '뚱딴지',\n",
       " '용인',\n",
       " '사주',\n",
       " '디렉팅',\n",
       " '삽겹살',\n",
       " '추심',\n",
       " '국회',\n",
       " '드',\n",
       " '루안',\n",
       " '임재욱',\n",
       " '차지연',\n",
       " '스케치북',\n",
       " '몽환',\n",
       " '주역',\n",
       " '도요토미',\n",
       " '소리치다',\n",
       " '지소연',\n",
       " '허이재',\n",
       " '학칙',\n",
       " '체게바라',\n",
       " '타치',\n",
       " '자취',\n",
       " '손색',\n",
       " '새다',\n",
       " '지부',\n",
       " '캐슬',\n",
       " '나비야',\n",
       " '구먼',\n",
       " '4시',\n",
       " '별루임',\n",
       " '냉정하다',\n",
       " '급박하다',\n",
       " '3등',\n",
       " '방귀뀐놈',\n",
       " '우상화',\n",
       " '썰렁하다',\n",
       " '스탠스',\n",
       " '윤성이',\n",
       " '보이',\n",
       " '광복군',\n",
       " '백령도',\n",
       " '이신',\n",
       " '데미지',\n",
       " '어지럽히다',\n",
       " '일변도',\n",
       " '임성한',\n",
       " '탁주',\n",
       " '감빵',\n",
       " '묘사',\n",
       " 'Yo',\n",
       " '볼맛',\n",
       " '범과',\n",
       " '죄짓',\n",
       " '마포대교',\n",
       " '아쥼마',\n",
       " '다리꼬',\n",
       " '유서',\n",
       " '성교육',\n",
       " '엇음',\n",
       " '예뿜',\n",
       " '탱크',\n",
       " '김갑수',\n",
       " '귀로',\n",
       " '바려',\n",
       " '시인',\n",
       " '출근',\n",
       " '얌전하다',\n",
       " '제거',\n",
       " '돈쫌',\n",
       " '경감',\n",
       " '풍선',\n",
       " 'y',\n",
       " '요부',\n",
       " '후후훗',\n",
       " '윤화언',\n",
       " '300',\n",
       " '비고',\n",
       " '네거티브',\n",
       " '차보',\n",
       " '38년',\n",
       " '뽕뱅',\n",
       " '구옥',\n",
       " '시구',\n",
       " '귀걸이',\n",
       " '달콤',\n",
       " '눈데',\n",
       " '리허설',\n",
       " '상주',\n",
       " '준강간',\n",
       " '양질',\n",
       " '단서',\n",
       " '첼시',\n",
       " '도망',\n",
       " '부각',\n",
       " '김소영',\n",
       " 'country',\n",
       " '오세훈',\n",
       " '려욱',\n",
       " '이소룡',\n",
       " '훈련',\n",
       " '라무',\n",
       " '늘씬',\n",
       " '야기',\n",
       " '눈망울',\n",
       " '매라',\n",
       " '역사영화',\n",
       " '상반기',\n",
       " '씨레기',\n",
       " '조사',\n",
       " '국밥',\n",
       " '걸스데이',\n",
       " 'drug',\n",
       " '관심병사',\n",
       " '똥똥',\n",
       " '전효진',\n",
       " '아라시',\n",
       " '구주',\n",
       " '여건',\n",
       " '극뽁',\n",
       " '소드',\n",
       " '초대권',\n",
       " '텃세',\n",
       " '체험',\n",
       " '에게선',\n",
       " '사운드',\n",
       " '갑상선암',\n",
       " '알바몬',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros(100, dtype=np.float32)\n",
    "    \n",
    "    num_words = 0\n",
    "    # 어휘 사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words = 1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07574642, -0.0988207 , -0.48576212, -0.4776301 ,  1.0557545 ,\n",
       "        1.638185  , -0.52016985, -0.10827143, -0.6731411 ,  0.5465597 ,\n",
       "       -1.2510335 ,  0.7781881 ,  0.86021054, -1.5637627 , -1.1958218 ,\n",
       "        0.48384106, -0.5856546 ,  0.3942331 ,  0.58857167, -0.8910247 ,\n",
       "       -0.86850727,  1.3836409 ,  0.14626695,  0.72884285,  0.14109324,\n",
       "       -0.9455564 , -1.2405579 , -0.9072786 ,  0.1578528 , -0.3662238 ,\n",
       "        0.9585849 ,  0.27965495, -0.5208655 ,  0.61863285,  0.80589837,\n",
       "       -0.7552631 , -0.7169019 , -0.13069305,  0.49273053,  0.58437   ,\n",
       "       -0.9490637 ,  0.21465805, -1.1149619 , -2.0163708 , -0.53309685,\n",
       "        1.2184591 , -0.40251458, -1.573397  ,  0.21312025, -0.6301665 ,\n",
       "       -0.09679359, -0.5476607 , -1.2148906 ,  0.9498155 ,  1.4136295 ,\n",
       "       -0.6151596 , -0.06888026, -0.26032174, -0.1605458 ,  1.1390946 ,\n",
       "        0.05094002, -0.39497793, -0.60097647, -0.42819184, -0.88350314,\n",
       "       -0.45874068,  0.06497264,  1.0659873 ,  0.55221194,  0.5469426 ,\n",
       "        0.96317333, -0.6499646 , -1.0855448 , -0.46252096,  1.7661914 ,\n",
       "       -0.51355326,  0.35093284, -0.7581463 , -0.70553327,  0.7492883 ,\n",
       "        0.84961945, -0.6709137 , -0.9692545 , -0.4976806 ,  0.39489126,\n",
       "       -0.7362197 ,  0.7014364 , -0.2662686 , -0.26643842,  0.7533455 ,\n",
       "        1.4570553 , -0.91731364, -0.7014527 , -0.1710713 ,  0.13824178,\n",
       "       -1.6610003 , -1.7594907 , -1.3787923 ,  0.22260062,  0.9479208 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features('아 진짜 짜증난다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(comments):\n",
    "    dataset = []\n",
    "    \n",
    "    for s in comments:\n",
    "        dataset.append(get_features(s))\n",
    "        \n",
    "    commentFeatureVecs = np.stack(dataset)\n",
    "    return commentFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vecs = get_dataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train_vecs = get_dataset(X_train)\n",
    "\n",
    "lr = LogisticRegression(multi_class='multinomial', class_weight='balanced')\n",
    "lr.fit(X_train_vecs, y_train)\n",
    "\n",
    "X_test_vecs = get_dataset(X_test)\n",
    "\n",
    "preds = lr.predict(X_test_vecs)\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5520169851380042 F1 Score : 0.5457512516497584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(\"Accuracy : {}\".format(accuracy_score(preds, y_test)),\n",
    "    \"F1 Score : {}\".format(f1_score(preds, y_test, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지현우 나쁜놈</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설마 ㅈ 현정 작가 아니지??</td>\n",
       "      <td>hate</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>지현우 범죄 저지르지 않았나요?</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>여자인생 망칠 일 있나 ㅋㅋ</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>할매젖x, 뱃살x, 몸매 s라인, 유륜은 적당해야됨(너무크거나 너무 작아도 x), ...</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments      label      preds\n",
       "0                          송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.       none       none\n",
       "1                                              지현우 나쁜놈  offensive       none\n",
       "2           알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라       hate       hate\n",
       "3                                     설마 ㅈ 현정 작가 아니지??       hate  offensive\n",
       "4    이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...  offensive       none\n",
       "..                                                 ...        ...        ...\n",
       "466                                  지현우 범죄 저지르지 않았나요?  offensive       none\n",
       "467                                    여자인생 망칠 일 있나 ㅋㅋ       hate       hate\n",
       "468            근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?  offensive       none\n",
       "469  할매젖x, 뱃살x, 몸매 s라인, 유륜은 적당해야됨(너무크거나 너무 작아도 x), ...       hate       hate\n",
       "470  남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...       none       hate\n",
       "\n",
       "[471 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['preds'] = preds\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지현우 나쁜놈</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설마 ㅈ 현정 작가 아니지??</td>\n",
       "      <td>hate</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>알았어 그만</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>아c발 어쩌라고 뭔기사가계속나오냐</td>\n",
       "      <td>hate</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>강용석도 찌질하네 과거들추기는 추하다</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>지겹이 이프로 그만하자 작가야</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>지현우 범죄 저지르지 않았나요?</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?</td>\n",
       "      <td>offensive</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments      label      preds\n",
       "1                                              지현우 나쁜놈  offensive       none\n",
       "3                                     설마 ㅈ 현정 작가 아니지??       hate  offensive\n",
       "4    이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...  offensive       none\n",
       "8                                               알았어 그만  offensive       none\n",
       "11                                  아c발 어쩌라고 뭔기사가계속나오냐       hate  offensive\n",
       "..                                                 ...        ...        ...\n",
       "463                               강용석도 찌질하네 과거들추기는 추하다  offensive       none\n",
       "464                                   지겹이 이프로 그만하자 작가야  offensive       none\n",
       "466                                  지현우 범죄 저지르지 않았나요?  offensive       none\n",
       "468            근데 전라도에서 사고가 났는데 굳이 서울까지 와서 병원에 가느 이유는?  offensive       none\n",
       "470  남자가 잘못한거라면... 반성도 없다면...나였다면 ... 여자처럼 아주 못되게 할...       none       hate\n",
       "\n",
       "[211 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['label'] != test['preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics                 import accuracy_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.svm                     import SVC\n",
    "from lightgbm                        import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RandomForestClassifier(), LogisticRegression(), SVC(), LGBMClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "Accuracy : 0.4437367303609342\n",
      "F1 Score : 0.40657653491000234\n",
      "\n",
      "LogisticRegression()\n",
      "Accuracy : 0.5201698513800425\n",
      "F1 Score : 0.49559043496640337\n",
      "\n",
      "SVC()\n",
      "Accuracy : 0.4819532908704883\n",
      "F1 Score : 0.451411480568302\n",
      "\n",
      "LGBMClassifier()\n",
      "Accuracy : 0.5031847133757962\n",
      "F1 Score : 0.48263028571247757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    vec_model = model\n",
    "\n",
    "    vec_model.fit(X_train_vecs, y_train)\n",
    "    preds = vec_model.predict(X_test_vecs)\n",
    "\n",
    "    print(model, \"\\n\",\n",
    "          \"Accuracy : {}\".format(accuracy_score(preds, y_test)), '\\n'\n",
    "          \"F1 Score : {}\".format(f1_score(preds, y_test, average='macro')), '\\n', sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./comments100.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./new_train.csv')\n",
    "dev = pd.read_csv('../../data/dev.hate.csv')\n",
    "test = pd.read_csv('../../data/test.hate.no_label.csv')\n",
    "train = pd.concat([train, dev], axis=0)\n",
    "X_train = train['comments']\n",
    "X_test = test['comments']\n",
    "y_train = train['label']\n",
    "# y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vecs = get_dataset(X_train)\n",
    "\n",
    "lr = LogisticRegression(multi_class='multinomial', class_weight='balanced')\n",
    "lr.fit(X_train_vecs, y_train)\n",
    "\n",
    "X_test_vecs = get_dataset(X_test)\n",
    "\n",
    "preds = lr.predict(X_test_vecs)\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>둘다 넘 좋다~행복하세요</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>장현승 얘도 참 이젠 짠하다...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comments label\n",
       "0  ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ  none\n",
       "1                                 둘다 넘 좋다~행복하세요  none\n",
       "2          근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데  none\n",
       "3         원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요  none\n",
       "4                            장현승 얘도 참 이젠 짠하다...  none"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'] = preds\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_format(df):\n",
    "    df['label'][df['label'] == 'none'] = 0\n",
    "    df['label'][df['label'] == 'offensive'] = 1\n",
    "    df['label'][df['label'] == 'hate'] = 2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>둘다 넘 좋다~행복하세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>장현승 얘도 참 이젠 짠하다...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>대박 게스트... 꼭 봐야징~ 컨셉이 바뀌니깐 재미지넹</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>성형으로 다 뜯어고쳐놓고 예쁜척. 성형 전 니 얼굴 다 알고있다. 순자처럼 된장냄새...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>분위기는 비슷하다만 전혀다른 전개던데 무슨ㅋㅋㄱ 우리나라사람들은 분위기만 비슷하면 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>입에 손가릭이 10개 있으니 징그럽다</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>난 조보아 이뻐서 보는데 백종원 관심무</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comments label\n",
       "0         ㅋㅋㅋㅋ 그래도 조아해주는 팬들 많아서 좋겠다 ㅠㅠ 니들은 온유가 안만져줌 ㅠㅠ     0\n",
       "1                                        둘다 넘 좋다~행복하세요     0\n",
       "2                 근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데     0\n",
       "3                원곡생각하나도 안나고 러블리즈 신곡나온줄!!! 너무 예쁘게 잘봤어요     0\n",
       "4                                   장현승 얘도 참 이젠 짠하다...     0\n",
       "..                                                 ...   ...\n",
       "969                     대박 게스트... 꼭 봐야징~ 컨셉이 바뀌니깐 재미지넹     2\n",
       "970  성형으로 다 뜯어고쳐놓고 예쁜척. 성형 전 니 얼굴 다 알고있다. 순자처럼 된장냄새...     2\n",
       "971  분위기는 비슷하다만 전혀다른 전개던데 무슨ㅋㅋㄱ 우리나라사람들은 분위기만 비슷하면 ...     2\n",
       "972                               입에 손가릭이 10개 있으니 징그럽다     2\n",
       "973                              난 조보아 이뻐서 보는데 백종원 관심무     0\n",
       "\n",
       "[974 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_format(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('./0117_jc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
