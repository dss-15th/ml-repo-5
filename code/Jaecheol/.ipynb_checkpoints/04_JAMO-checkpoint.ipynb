{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics                 import accuracy_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.svm                     import SVC\n",
    "from lightgbm                        import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./new_train.csv')\n",
    "test = pd.read_csv('../../data/dev.hate.csv')\n",
    "train = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['comments']\n",
    "X_test = test['comments']\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모 / ngram / 모델 GridSearch (전처리X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅅㅂ ㅁㅗㄹㅡㄱㅗㅈㅓㄹㅓㄴㄱㅓㅈㅣ ㅇㅏㄹㄱㅗ ㅈㅓㄹㅐㅆㄱㅔㅆㄴㅑ ㄲㅗㄴㄷㅐ ㅅㅐㅋㅣㄷㅡㄹ ㄷㅡㄹㅓㅂㄱㅔ ㅁㅏㄹㅁㅏㄶㄴㅔ ㅋㅋㅋ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jamo import h2j, j2hcj\n",
    "\n",
    "def jamo_tokenizer(text):\n",
    "    return j2hcj(h2j(text))\n",
    "\n",
    "jamo_tokenizer(train['comments'][184])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), RandomForestClassifier(), SVC(), LGBMClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : LogisticRegression() ngram : (1, 2) \n",
      " ['train : 0.6289701548621682', 'test : 0.5407363740006657']\n",
      "model : LogisticRegression() ngram : (1, 3) \n",
      " ['train : 0.7530548922610246', 'test : 0.5598319803320194']\n",
      "model : LogisticRegression() ngram : (1, 4) \n",
      " ['train : 0.8285115749991395', 'test : 0.5637178344327568']\n",
      "model : LogisticRegression() ngram : (1, 5) \n",
      " ['train : 0.866858004759186', 'test : 0.558321573749179']\n",
      "model : LogisticRegression() ngram : (1, 6) \n",
      " ['train : 0.884342531760867', 'test : 0.5516118479375248']\n",
      "model : LogisticRegression() ngram : (1, 7) \n",
      " ['train : 0.896918039194181', 'test : 0.5640981120927043']\n",
      "model : LogisticRegression() ngram : (1, 8) \n",
      " ['train : 0.904529165835803', 'test : 0.5542973971848706']\n",
      "model : LogisticRegression() ngram : (1, 9) \n",
      " ['train : 0.9059971161877013', 'test : 0.5521487406139399']\n",
      "model : LogisticRegression() ngram : (1, 10) \n",
      " ['train : 0.9078608155387831', 'test : 0.549361132694466']\n",
      "model : LogisticRegression() ngram : (1, 11) \n",
      " ['train : 0.9084605453417546', 'test : 0.5530184827764711']\n",
      "model : LogisticRegression() ngram : (1, 12) \n",
      " ['train : 0.9088130789356622', 'test : 0.5511772470386548']\n",
      "model : RandomForestClassifier() ngram : (1, 2) \n",
      " ['train : 1.0', 'test : 0.4102215097772281']\n",
      "model : RandomForestClassifier() ngram : (1, 3) \n",
      " ['train : 1.0', 'test : 0.4635095694035178']\n",
      "model : RandomForestClassifier() ngram : (1, 4) \n",
      " ['train : 1.0', 'test : 0.4621858906902143']\n",
      "model : RandomForestClassifier() ngram : (1, 5) \n",
      " ['train : 1.0', 'test : 0.49426713601251593']\n",
      "model : RandomForestClassifier() ngram : (1, 6) \n",
      " ['train : 1.0', 'test : 0.4721091460281439']\n",
      "model : RandomForestClassifier() ngram : (1, 7) \n",
      " ['train : 1.0', 'test : 0.45681906169711045']\n",
      "model : RandomForestClassifier() ngram : (1, 8) \n",
      " ['train : 1.0', 'test : 0.481822218439459']\n",
      "model : RandomForestClassifier() ngram : (1, 9) \n",
      " ['train : 1.0', 'test : 0.4968105945640858']\n",
      "model : RandomForestClassifier() ngram : (1, 10) \n",
      " ['train : 1.0', 'test : 0.5149463357370451']\n",
      "model : RandomForestClassifier() ngram : (1, 11) \n",
      " ['train : 1.0', 'test : 0.4997676606577972']\n",
      "model : RandomForestClassifier() ngram : (1, 12) \n",
      " ['train : 1.0', 'test : 0.4891842385714699']\n",
      "model : SVC() ngram : (1, 2) \n",
      " ['train : 0.8460217737789284', 'test : 0.5256891739515087']\n",
      "model : SVC() ngram : (1, 3) \n",
      " ['train : 0.9441100406818407', 'test : 0.5247080350076138']\n",
      "model : SVC() ngram : (1, 4) \n",
      " ['train : 0.9755721076592376', 'test : 0.5387635937905446']\n",
      "model : SVC() ngram : (1, 5) \n",
      " ['train : 0.9846024180046551', 'test : 0.543571440859532']\n",
      "model : SVC() ngram : (1, 6) \n",
      " ['train : 0.9869358599059893', 'test : 0.5455315055315055']\n",
      "model : SVC() ngram : (1, 7) \n",
      " ['train : 0.9890984571779747', 'test : 0.5419083171257085']\n",
      "model : SVC() ngram : (1, 8) \n",
      " ['train : 0.9895793670249478', 'test : 0.5397377154444352']\n",
      "model : SVC() ngram : (1, 9) \n",
      " ['train : 0.9900045602224526', 'test : 0.5397428353098085']\n",
      "model : SVC() ngram : (1, 10) \n",
      " ['train : 0.9901801836857475', 'test : 0.5429823420489448']\n",
      "model : SVC() ngram : (1, 11) \n",
      " ['train : 0.9902948059065416', 'test : 0.5400368259191789']\n",
      "model : SVC() ngram : (1, 12) \n",
      " ['train : 0.990409414524921', 'test : 0.5382270105678147']\n",
      "model : LGBMClassifier() ngram : (1, 2) \n",
      " ['train : 0.9395910859926749', 'test : 0.4848589734094846']\n",
      "model : LGBMClassifier() ngram : (1, 3) \n",
      " ['train : 0.9467192480776124', 'test : 0.5404467799631484']\n",
      "model : LGBMClassifier() ngram : (1, 4) \n",
      " ['train : 0.9513672743025868', 'test : 0.5457642464102926']\n",
      "model : LGBMClassifier() ngram : (1, 5) \n",
      " ['train : 0.9523009431117163', 'test : 0.5428494261790916']\n",
      "model : LGBMClassifier() ngram : (1, 6) \n",
      " ['train : 0.9545856703714168', 'test : 0.5547562213174361']\n",
      "model : LGBMClassifier() ngram : (1, 7) \n",
      " ['train : 0.9541507861760339', 'test : 0.5379878275094495']\n",
      "model : LGBMClassifier() ngram : (1, 8) \n",
      " ['train : 0.9550730571208929', 'test : 0.5245729743399213']\n",
      "model : LGBMClassifier() ngram : (1, 9) \n",
      " ['train : 0.9556461061349788', 'test : 0.5507822313577709']\n",
      "model : LGBMClassifier() ngram : (1, 10) \n",
      " ['train : 0.9548455481337313', 'test : 0.5613433994247948']\n",
      "model : LGBMClassifier() ngram : (1, 11) \n",
      " ['train : 0.9558611018605552', 'test : 0.5405157108055659']\n",
      "model : LGBMClassifier() ngram : (1, 12) \n",
      " ['train : 0.9566472813036447', 'test : 0.5619999091155129']\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for ngram in range(2, 13):\n",
    "        vec_pipe = Pipeline([\n",
    "                            (\"vec\", TfidfVectorizer(tokenizer=jamo_tokenizer)), \n",
    "                            (\"model\", model)\n",
    "                            ])\n",
    "        # Setting the VEC hyperparameters\n",
    "        vec_pipe_params = {\"vec__ngram_range\" : [(1,ngram)], \n",
    "                            \"vec__stop_words\"  : [None],\n",
    "                            \"vec__min_df\" : [3],\n",
    "                            \"vec__max_df\" : [0.9]}  \n",
    "\n",
    "        # Instantiating the grid search\n",
    "        vec_gs = GridSearchCV(vec_pipe,\n",
    "                                param_grid=vec_pipe_params,\n",
    "                                cv=3)\n",
    "\n",
    "        vec_gs.fit(X_train, y_train);\n",
    "        train_pred = vec_gs.predict(X_train)\n",
    "        test_pred = vec_gs.predict(X_test)\n",
    "        result = [\"train : {}\".format(f1_score(train_pred, y_train, average='macro')),\n",
    "                \"test : {}\".format(f1_score(test_pred, y_test, average='macro'))]\n",
    "        print(\"model : {}\".format(model), \"ngram : (1, {})\".format(ngram), '\\n', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅅㅅ 한게 자랑  ㅈㅅ로 그것도\n",
      "ㅅㅅ 한 게 자랑 ㅈㅅ로 그것도\n",
      "ㅅㅅ 한 게 자랑 ㅈㅅ로 그것도\n",
      "ㅅㅅ 한 게 자랑 ㅈㅅ로 그것도\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 제거\n",
    "import re\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)\n",
    "    return text\n",
    "train['comments'] = train['comments'].apply(cleanse)\n",
    "print(train['comments'][185])\n",
    "\n",
    "# 띄어쓰기\n",
    "from pykospacing import spacing\n",
    "train['comments'] = train['comments'].apply(spacing)\n",
    "print(train['comments'][185])\n",
    "\n",
    "# 문장 분리\n",
    "import kss\n",
    "train['comments'] = train['comments'].apply(kss.split_sentences)\n",
    "train['comments'] = [','.join(map(str, ls)) for ls in train['comments']]\n",
    "print(train['comments'][185])\n",
    "\n",
    "# 중복 제거\n",
    "from soynlp.normalizer import *\n",
    "train['comments'] = [repeat_normalize(comment, num_repeats=2) for comment in train['comments']]\n",
    "print(train['comments'][185])\n",
    "\n",
    "X_train = train['comments']\n",
    "X_test = test['comments']\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모, 로지스틱 파라미터 튜닝, ngram 조절(전처리 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram : (1, 2) ['train : 0.6332185142154709', 'test : 0.5300778301980638']\n",
      "ngram : (1, 3) ['train : 0.769700696503487', 'test : 0.5844183813990914']\n",
      "ngram : (1, 4) ['train : 0.8507049741827865', 'test : 0.6130502867121838']\n",
      "ngram : (1, 5) ['train : 0.8890713994695544', 'test : 0.6000741886614462']\n",
      "ngram : (1, 6) ['train : 0.9104170772016915', 'test : 0.611143258597239']\n",
      "ngram : (1, 7) ['train : 0.9222442600578363', 'test : 0.6082232272176408']\n",
      "ngram : (1, 8) ['train : 0.9262657366998369', 'test : 0.6053783665243234']\n",
      "ngram : (1, 9) ['train : 0.9291297037844443', 'test : 0.6056927598845108']\n",
      "ngram : (1, 10) ['train : 0.9299383291922437', 'test : 0.6036794415557777']\n"
     ]
    }
   ],
   "source": [
    "for ngram in range(2, 11):\n",
    "    vec_pipe = Pipeline([\n",
    "                        (\"vec\", TfidfVectorizer(tokenizer=jamo_tokenizer)), \n",
    "                        (\"model\", LogisticRegression(multi_class='multinomial', class_weight='balanced'))\n",
    "                        ])\n",
    "\n",
    "    # Setting the VEC hyperparameters\n",
    "    vec_pipe_params = {\"vec__ngram_range\" : [(1,ngram)], \n",
    "                        \"vec__stop_words\"  : [None],\n",
    "                        \"vec__min_df\" : [3],\n",
    "                        \"vec__max_df\" : [0.9]}    \n",
    "\n",
    "    # Instantiating the grid search\n",
    "    vec_gs = GridSearchCV(vec_pipe,\n",
    "                            param_grid=vec_pipe_params,\n",
    "                            cv=3)\n",
    "\n",
    "    vec_gs.fit(X_train, y_train)\n",
    "    train_pred = vec_gs.predict(X_train)\n",
    "    test_pred = vec_gs.predict(X_test)\n",
    "    result = [\"train : {}\".format(f1_score(train_pred, y_train, average='macro')),\n",
    "              \"test : {}\".format(f1_score(test_pred, y_test, average='macro'))]\n",
    "    print(\"ngram : (1, {})\".format(ngram), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모, 로지스틱 파라미터 튜닝, ngram 조절(전처리 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram : (1, 4) ['train : 0.8373754539260437', 'test : 0.5809322350063493']\n",
      "ngram : (1, 5) ['train : 0.8834284132587712', 'test : 0.59977834464169']\n",
      "ngram : (1, 6) ['train : 0.908548067048668', 'test : 0.6178219515229056']\n",
      "ngram : (1, 7) ['train : 0.9216788971015873', 'test : 0.6170750919932341']\n",
      "ngram : (1, 8) ['train : 0.9273456873822958', 'test : 0.6084726096755447']\n",
      "ngram : (1, 9) ['train : 0.9313901774887348', 'test : 0.6098143611817685']\n",
      "ngram : (1, 10) ['train : 0.9332830210833913', 'test : 0.6081627365867254']\n"
     ]
    }
   ],
   "source": [
    "for ngram in range(4, 11):\n",
    "    vec_pipe = Pipeline([\n",
    "                        (\"vec\", TfidfVectorizer(tokenizer=jamo_tokenizer)), \n",
    "                        (\"model\", LogisticRegression(multi_class='multinomial', class_weight='balanced'))\n",
    "                        ])\n",
    "\n",
    "    # Setting the VEC hyperparameters\n",
    "    vec_pipe_params = {\"vec__ngram_range\" : [(1,ngram)], \n",
    "                        \"vec__stop_words\"  : [None],\n",
    "                        \"vec__min_df\" : [3],\n",
    "                        \"vec__max_df\" : [0.9]}    \n",
    "\n",
    "    # Instantiating the grid search\n",
    "    vec_gs = GridSearchCV(vec_pipe,\n",
    "                            param_grid=vec_pipe_params,\n",
    "                            cv=3)\n",
    "\n",
    "    vec_gs.fit(X_train, y_train)\n",
    "    train_pred = vec_gs.predict(X_train)\n",
    "    test_pred = vec_gs.predict(X_test)\n",
    "    result = [\"train : {}\".format(f1_score(train_pred, y_train, average='macro')),\n",
    "              \"test : {}\".format(f1_score(test_pred, y_test, average='macro'))]\n",
    "    print(\"ngram : (1, {})\".format(ngram), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모, 로지스틱/TFIDF 파라미터 튜닝, ngram 조절(전처리 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram : (1, 4) ['train : 0.8480346912201067', 'test : 0.5775456651925654']\n",
      "ngram : (1, 5) ['train : 0.905544460453064', 'test : 0.5897255419745057']\n",
      "ngram : (1, 6) ['train : 0.9425901957347599', 'test : 0.5908834564185872']\n",
      "ngram : (1, 7) ['train : 0.9591581153454053', 'test : 0.5913136021800683']\n",
      "ngram : (1, 8) ['train : 0.9718227601156647', 'test : 0.5923780631678296']\n",
      "ngram : (1, 9) ['train : 0.9794762408269505', 'test : 0.5830090456980482']\n",
      "ngram : (1, 10) ['train : 0.983741479262577', 'test : 0.5739632782219531']\n"
     ]
    }
   ],
   "source": [
    "for ngram in range(4, 11):\n",
    "    vec_pipe = Pipeline([\n",
    "                        (\"vec\", TfidfVectorizer(tokenizer=jamo_tokenizer)), \n",
    "                        (\"model\", LogisticRegression(multi_class='multinomial', class_weight='balanced'))\n",
    "                        ])\n",
    "\n",
    "    # Setting the VEC hyperparameters\n",
    "    vec_pipe_params = {\"vec__ngram_range\" : [(1,ngram)], \n",
    "                        \"vec__stop_words\"  : [None],\n",
    "#                         \"vec__min_df\" : [3],\n",
    "#                         \"vec__max_df\" : [0.9]\n",
    "                      }    \n",
    "\n",
    "    # Instantiating the grid search\n",
    "    vec_gs = GridSearchCV(vec_pipe,\n",
    "                            param_grid=vec_pipe_params,\n",
    "                            cv=3)\n",
    "\n",
    "    vec_gs.fit(X_train, y_train)\n",
    "    train_pred = vec_gs.predict(X_train)\n",
    "    test_pred = vec_gs.predict(X_test)\n",
    "    result = [\"train : {}\".format(f1_score(train_pred, y_train, average='macro')),\n",
    "              \"test : {}\".format(f1_score(test_pred, y_test, average='macro'))]\n",
    "    print(\"ngram : (1, {})\".format(ngram), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
