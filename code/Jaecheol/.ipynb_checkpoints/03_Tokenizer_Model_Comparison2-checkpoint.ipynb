{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes             import MultinomialNB\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics                 import accuracy_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.svm                     import SVC\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from lightgbm                        import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./new_train.csv')\n",
    "test = pd.read_csv('../../data/dev.hate.csv')\n",
    "train = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅅㅂ 모르고저런거지 알고 저랬겠냐 꼰대 새키들 드럽게 말많네 ㅋㅋㅋ\n",
      "ㅅㅂ 모르고 저런 거지 알고 저랬겠냐 꼰대 새키들 드럽게 말 많네 ㅋㅋㅋ\n",
      "ㅅㅂ 모르고 저런 거지 알고 저랬겠냐 꼰대 새키들 드럽게 말 많네 ㅋㅋㅋ\n",
      "ㅅㅂ 모르고 저런 거지 알고 저랬겠냐 꼰대 새키들 드럽게 말 많네 ㅋㅋㅋ\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 제거\n",
    "import re\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)\n",
    "    return text\n",
    "train['comments'] = train['comments'].apply(cleanse)\n",
    "print(train['comments'][184])\n",
    "\n",
    "# 띄어쓰기\n",
    "from pykospacing import spacing\n",
    "train['comments'] = train['comments'].apply(spacing)\n",
    "print(train['comments'][184])\n",
    "\n",
    "# 문장 분리\n",
    "import kss\n",
    "train['comments'] = train['comments'].apply(kss.split_sentences)\n",
    "train['comments'] = [','.join(map(str, ls)) for ls in train['comments']]\n",
    "print(train['comments'][184])\n",
    "\n",
    "# 중복 제거\n",
    "from soynlp.normalizer import *\n",
    "train['comments'] = [repeat_normalize(comment, num_repeats=2) for comment in train['comments']]\n",
    "print(train['comments'][184])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기별 토크나이저 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi; k = KhaiiiApi()\n",
    "from konlpy.tag import Okt; t = Okt()\n",
    "from konlpy.tag import Mecab; m = Mecab()\n",
    "\n",
    "# khai\n",
    "def k_tokenizer(text):\n",
    "    k.analyze(text)\n",
    "    return [word.lex for word in k.analyze(text)]\n",
    "\n",
    "# Okt\n",
    "def t_tokenizer(text):\n",
    "    tokens_ko = t.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "# Mecab\n",
    "def m_tokenizer(text):\n",
    "    tokens_ko = m.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델, 토크나이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(), RandomForestClassifier(), SVC(), LGBMClassifier()]\n",
    "tokenizers = [None, k_tokenizer, t_tokenizer, m_tokenizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['comments']\n",
    "X_test = test['comments']\n",
    "y_train = train['label']\n",
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델별/토큰별 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, tokenizer):\n",
    "    # 훈련: train 전체 / 테스트: dev 전체\n",
    "    X_train = train['comments']\n",
    "    X_test = test['comments']\n",
    "    y_train = train['label']\n",
    "    y_test = test['label']\n",
    "    \n",
    "    # Setting up the pipeline\n",
    "    vec_pipe = Pipeline([\n",
    "                        (\"vec\", TfidfVectorizer(tokenizer=tokenizer)), \n",
    "                        (\"model\", model)\n",
    "                        ])\n",
    "    \n",
    "    # Setting the VEC hyperparameters\n",
    "    vec_pipe_params = {\"vec__ngram_range\" : [(1,2)], \n",
    "                       \"vec__stop_words\"  : [None],\n",
    "                       \"vec__min_df\" : [3],\n",
    "                       \"vec__max_df\" : [0.9]}    \n",
    "    \n",
    "    # Instantiating the grid search\n",
    "    vec_gs = GridSearchCV(vec_pipe,\n",
    "                          param_grid=vec_pipe_params,\n",
    "                          cv=3)\n",
    "\n",
    "    # Fitting the model to the training data\n",
    "    vec_gs.fit(X_train, y_train);\n",
    "    \n",
    "    # Predicting\n",
    "    train_pred = vec_gs.predict(X_train)\n",
    "    test_pred = vec_gs.predict(X_test)\n",
    "\n",
    "    # Score\n",
    "    result = [\"train : \", f1_score(train_pred, y_train, average='macro'),\n",
    "              \"test : \", f1_score(test_pred, y_test, average='macro')]    \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : LogisticRegression()\n",
      "Tokenizer : None\n",
      "['train : ', 0.7561002272617734, 'test : ', 0.4199712324941378]\n",
      "Model : LogisticRegression()\n",
      "Tokenizer : k_tokenizer\n",
      "['train : ', 0.7798562593904027, 'test : ', 0.41844679583362815]\n",
      "Model : LogisticRegression()\n",
      "Tokenizer : t_tokenizer\n",
      "['train : ', 0.8322829113342763, 'test : ', 0.579778334351531]\n",
      "Model : LogisticRegression()\n",
      "Tokenizer : m_tokenizer\n",
      "['train : ', 0.8413548351609329, 'test : ', 0.5403916839167854]\n",
      "Model : RandomForestClassifier()\n",
      "Tokenizer : None\n",
      "['train : ', 0.9664996096631214, 'test : ', 0.36422576727970696]\n",
      "Model : RandomForestClassifier()\n",
      "Tokenizer : k_tokenizer\n",
      "['train : ', 0.9844063126867181, 'test : ', 0.36234109656514923]\n",
      "Model : RandomForestClassifier()\n",
      "Tokenizer : t_tokenizer\n",
      "['train : ', 0.9986766860902726, 'test : ', 0.5051114757421531]\n",
      "Model : RandomForestClassifier()\n",
      "Tokenizer : m_tokenizer\n",
      "['train : ', 0.999328139864184, 'test : ', 0.5008706184025419]\n",
      "Model : SVC()\n",
      "Tokenizer : None\n",
      "['train : ', 0.9280831729717262, 'test : ', 0.3806121790337902]\n",
      "Model : SVC()\n",
      "Tokenizer : k_tokenizer\n",
      "['train : ', 0.9481485847730968, 'test : ', 0.3864338075235036]\n",
      "Model : SVC()\n",
      "Tokenizer : t_tokenizer\n",
      "['train : ', 0.9739947838977843, 'test : ', 0.5362595234121097]\n",
      "Model : SVC()\n",
      "Tokenizer : m_tokenizer\n",
      "['train : ', 0.9805786272282445, 'test : ', 0.5277057631563263]\n",
      "Model : LGBMClassifier()\n",
      "Tokenizer : None\n",
      "['train : ', 0.5984332719605011, 'test : ', 0.34790252639517344]\n",
      "Model : LGBMClassifier()\n",
      "Tokenizer : k_tokenizer\n",
      "['train : ', 0.6547420380326734, 'test : ', 0.34662971268663867]\n",
      "Model : LGBMClassifier()\n",
      "Tokenizer : t_tokenizer\n",
      "['train : ', 0.7879649438259063, 'test : ', 0.5462737479349814]\n",
      "Model : LGBMClassifier()\n",
      "Tokenizer : m_tokenizer\n",
      "['train : ', 0.8272281765160514, 'test : ', 0.5097475673692166]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    for tokenizer in tokenizers:\n",
    "        results.append(get_score(model, tokenizer))\n",
    "        \n",
    "        if tokenizer == None:\n",
    "            print(\"Model : {}\".format(model),\n",
    "                  \"Tokenizer : None\",\n",
    "                  get_score(model, tokenizer),\n",
    "                  sep='\\n')\n",
    "        else:\n",
    "            print(\"Model : {}\".format(model),\n",
    "                  'Tokenizer : {}'.format(tokenizer.__name__),\n",
    "                  get_score(model, tokenizer),\n",
    "                  sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 미적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 None / 음절 단위 / ngram(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_pipe = Pipeline([\n",
    "                    (\"vec\", TfidfVectorizer(tokenizer=None, analyzer='char')), \n",
    "                    (\"model\", LogisticRegression())\n",
    "                    ])\n",
    "    \n",
    "    # Setting the VEC hyperparameters\n",
    "vec_pipe_params = {\"vec__ngram_range\" : [(1,2)], \n",
    "                    \"vec__stop_words\"  : [None],\n",
    "                    \"vec__min_df\" : [3],\n",
    "                    \"vec__max_df\" : [0.9]}    \n",
    "    \n",
    "    # Instantiating the grid search\n",
    "vec_gs = GridSearchCV(vec_pipe,\n",
    "                        param_grid=vec_pipe_params,\n",
    "                        cv=3)\n",
    "\n",
    "vec_gs.fit(X_train, y_train);\n",
    "train_pred = vec_gs.predict(X_train)\n",
    "test_pred = vec_gs.predict(X_test)\n",
    "result = [\"train : \", f1_score(train_pred, y_train, average='macro'),\n",
    "        \"test : \", f1_score(test_pred, y_test, average='macro')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train : ', 0.79848958223854, 'test : ', 0.5648313726340545]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 None / 음절 단위 / ngram(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_pipe = Pipeline([\n",
    "                    (\"vec\", TfidfVectorizer(analyzer='char')), \n",
    "                    (\"model\", LogisticRegression())\n",
    "                    ])\n",
    "    \n",
    "    # Setting the VEC hyperparameters\n",
    "vec_pipe_params = {\"vec__ngram_range\" : [(1,3)], \n",
    "                    \"vec__stop_words\"  : [None],\n",
    "                    \"vec__min_df\" : [3],\n",
    "                    \"vec__max_df\" : [0.9]}    \n",
    "    \n",
    "    # Instantiating the grid search\n",
    "vec_gs = GridSearchCV(vec_pipe,\n",
    "                        param_grid=vec_pipe_params,\n",
    "                        cv=3)\n",
    "\n",
    "vec_gs.fit(X_train, y_train);\n",
    "train_pred = vec_gs.predict(X_train)\n",
    "test_pred = vec_gs.predict(X_test)\n",
    "result = [\"train : \", f1_score(train_pred, y_train, average='macro'),\n",
    "        \"test : \", f1_score(test_pred, y_test, average='macro')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train : ', 0.858892743897349, 'test : ', 0.5551886385219719]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
